{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "N5C0qxlRVnDa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hwUShkMdh4ry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99108260-6c07-46a1-9702-06b3b7d49cf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "'/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/model' -> './model'\n",
            "'/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/vect' -> './vect'\n"
          ]
        }
      ],
      "source": [
        "### import docs pickle from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "! cp -r --verbose '/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/model' .\n",
        "! cp -r --verbose '/content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/vect' .\n",
        "\n",
        "MODEL_FILE = './model'\n",
        "VECT_FILE = './vect'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pk"
      ],
      "metadata": {
        "id": "_ePc7ecrm2_5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open docs pickle\n",
        "\n",
        "model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "print(vect)"
      ],
      "metadata": {
        "id": "faDm5bN3nm3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5f68325-b7e9-4138-830d-facfbb638473"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfVectorizer(max_df=0.36, min_df=0.05,\n",
            "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
            "                                      'afterwards', 'again', 'against', 'all',\n",
            "                                      'almost', 'alone', 'along', 'already',\n",
            "                                      'also', 'although', 'always', 'am',\n",
            "                                      'among', 'amongst', 'amoungst', 'amount',\n",
            "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
            "                                      'anyone', 'anything', 'anyway',\n",
            "                                      'anywhere', ...}))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = [\"Textblob is kinda bad to use. I don't like sushi but the ambiance was ok\"]"
      ],
      "metadata": {
        "id": "xgqLdkG6tKUe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polarité"
      ],
      "metadata": {
        "id": "ffojAzz47qtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "testimonial1 = TextBlob(str(TEXT))\n",
        "print(str(TEXT))\n",
        "print(testimonial1.sentiment.polarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reVAqacrqKXg",
        "outputId": "3561108b-c0c2-4b15-f3eb-5dc7b4dcc433"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Textblob is kinda bad to use. I don't like sushi but the ambiance was ok\"]\n",
            "-0.09999999999999992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def negative(text):\n",
        "  negative = False\n",
        "  testimonial1 = TextBlob(text)\n",
        "  if testimonial1.sentiment.polarity < 0:\n",
        "    negative = True\n",
        "  return negative "
      ],
      "metadata": {
        "id": "uo2-VFf96TVG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative(\"wasn't very pretty today, so I will not come a day like today\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yej9ygDm7BzS",
        "outputId": "3b68df56-a044-4f93-a5fb-79a23c45336c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prétraitement"
      ],
      "metadata": {
        "id": "Olc_vWh14aBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "!pip3 install contractions\n",
        "import contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjyd9dx_5zn6",
        "outputId": "dcf437d9-393f-40c9-a506-5da9ef86595f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 33.1 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 73.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed"
      ],
      "metadata": {
        "id": "xcpDDrvu4ncr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_text(\"wasn't very pretty today, so I will not come a day like today\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fBt-c61g5RVt",
        "outputId": "79fcb2b4-3cc9-417a-a5d7-a183b02a1dd1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wasn t very pretty today so I will not come a day like today'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7f5nOqA4x8b",
        "outputId": "d74ef783-00df-4607-c515-a162f466024f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lemmatize_text(\"The brown fox is quick and he is jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iubH4ULO5NrT",
        "outputId": "d6206237-35d9-4712-ad8e-7e0d17791333"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.66 s, sys: 48.8 ms, total: 1.71 s\n",
            "Wall time: 1.99 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox be quick and he be jump over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "id": "EAEYAG2B5DJz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_text(\"The brown fox is quick and he is jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lPJFsOJq5K0D",
        "outputId": "e93bc361-d733-44b0-cb92-8fd6f603fec4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the brown fox is quick and he is jumping over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "XwMvCiCI5Fcb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_text(\"The brown fox isn't quick and he wouldn't jumping over the lazy dog\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vwr_l19J5JiL",
        "outputId": "29c30ab2-b5ec-4b83-cdfe-b09d0856a107"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The brown fox is not quick and he would not jumping over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\""
      ],
      "metadata": {
        "id": "aFEfssn05UK8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "FqmoTiFp5VI0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_negative_token(\"I will never do that again, because I will not be foolish again, there is no risk, never\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "joxHB7G85WK8",
        "outputId": "ae3a1a74-02dc-471c-fe3d-afd71bd0e8db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I will NOT_do that again, because I will NOT_be foolish again, there is NOT_risk, never'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])"
      ],
      "metadata": {
        "id": "jKWTr7TY5Xmk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_stopwords(\"I will never do that again, because I will not be foolish again, there is no risk, never\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IE092_D85Y48",
        "outputId": "0799e243-13fd-4014-fd16-97b48a6384be"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I again, I foolish again, risk,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Préprocess final"
      ],
      "metadata": {
        "id": "_obHuJfd5b08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = tokenize_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    text = normalize_text(text)\n",
        "    text = contraction_text(text)\n",
        "    text = get_negative_token(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "oxedCb5r5bN8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = preprocess_text(\"I will never do that again, because I will not be foolish again, there is no risk, never\")\n",
        "print(text_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU2B4zw968_7",
        "outputId": "8e319574-e00e-4415-df64-5800a1821e2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOT_do NOT_be foolish NOT_risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# vectorization"
      ],
      "metadata": {
        "id": "FLG2LCX24igr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "my_vertorizer = vect.transform(TEXT)\n",
        "my_model = model.transform(my_vertorizer)\n",
        "\n",
        "print(np.argsort(my_model))\n",
        "print(my_model[0][0], my_model[0][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq21xxX4r2Ge",
        "outputId": "8e3b909e-3e06-40f8-bfb9-4d27de968f48"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]]\n",
            "0.01652309329991388 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_nuns = ['atmosphere_sound', 'chicken_menu', 'bad_service', 'pizza_menu', 'delivery', 'long_wait', 'drinks', 'wrong_marketing', 'dirty', 'rude_staff', 'burger_menu', 'over_priced', 'not_tasty', 'not_accessible', 'seasoning']\n",
        "nb_topic = 2"
      ],
      "metadata": {
        "id": "hNgPywdyAqsL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_nuns[i[-j]]"
      ],
      "metadata": {
        "id": "O0fuwJ86bENK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.argsort(my_model)) #list des topic les plus probable dans l'ordre décroissant\n",
        "print(my_model[0][0], my_model[0][-1]) #comparaison du dernier(0) et du premier(-1)\n",
        "for i in np.argsort(my_model): #inversement\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDJsr_WBBUba",
        "outputId": "abbfe464-9c37-4204-bae9-29f3ffa749a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]]\n",
            "0.01652309329991388 0.0\n",
            "[ 1  5  6  9 13 14  2 10 12  3  7  8 11  4  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "\n",
        "def topic_search(text, nb_topic, model, vectorizer):\n",
        "  text = [text]\n",
        "  my_vertorizer = vect.transform(text)\n",
        "  my_model = model.transform(my_vertorizer)\n",
        "  topics = []\n",
        "  for i in np.argsort(my_model):\n",
        "    print(i)\n",
        "    for j in range(nb_topic):\n",
        "      topics.append(topic_nuns[i[-j-1]])\n",
        "  return topics"
      ],
      "metadata": {
        "id": "VlfdaAsNEKsi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = \"My wife and I stopped in for a quick meal on a Tuesday night at 9:30. The place had 2 tables going. We sat in booths and were told to order quickly as the kitchen was closing (no problem).  Our booth was across from no less than 6 other booths and tables filled with food from un-bussed tables (big problem). To our shock, waiter after waiter walked by table after table and cleared nothing. The food was awful. We sent back a glass of wine that had smelled like vinegar. The bartender made a point of telling the waiter that it was a brand new opened bottle instead of simply pouring a good glass and leaving it at that. The bar had staff yucking it up loudly instead of cleaning. This place seriously lacks real management. Where's Gordon Ramsey when you need him? Good luck to all who dare tread here!\"\n",
        "text2 = \"So I recently moved to this area and was looking for a decent Chinese food place. This was not it. I ordered Mongolian chicken and house fried rice with hot and sour soup. Everything was disgusting. The hot and sour soup wasn't even a little hot. The Mongolian chicken had almost no flavor to it and certainly needed some seasoning and the rice....tasted like it was in the cooler for days. I will never order from here again, ended up eating PB&J that night and tossed everything in the trash. Don't waste your money.\""
      ],
      "metadata": {
        "id": "1DEwCrDfbUBm"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(topic_search(TEXT, 3, model=model, vectorizer=vect)))\n",
        "print(topic_search(text2, 3, model=model, vectorizer=vect))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7womXx8uFF8A",
        "outputId": "81688dee-c70f-44a0-c78f-9bb1d629174e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 3 11 12 10  1  4  0  2 13 14  5  7  8  9  6]\n",
            "<class 'list'>\n",
            "[ 3  5  8  9 10 11 13 14  2  4  7 12  0  6  1]\n",
            "['chicken_menu', 'drinks', 'atmosphere_sound']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation en fichier .py"
      ],
      "metadata": {
        "id": "Rhu81GgvVGd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/drive/MyDrive/ESEO/I3/S9/Langage_naturel/TP_issou/appp.py\n",
        "\n",
        "MODEL_FILE = './model'\n",
        "VECT_FILE = './vect'\n",
        "\n",
        "\n",
        "#polarite\n",
        "\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def negative(text):\n",
        "  negative = False\n",
        "  testimonial1 = TextBlob(text)\n",
        "  if testimonial1.sentiment.polarity < 0:\n",
        "    negative = True\n",
        "  return negative \n",
        "\n",
        "\n",
        "#pretraitement\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import contractions\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    return \" \".join(lemmatized_text_list)\n",
        "\n",
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])\n",
        "\n",
        "def contraction_text(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\"\n",
        "\n",
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = tokenize_text(text)\n",
        "    text = lemmatize_text(text)\n",
        "    text = normalize_text(text)\n",
        "    text = contraction_text(text)\n",
        "    text = get_negative_token(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#vectorization\n",
        "\n",
        "\n",
        "import pickle as pk\n",
        "import numpy as np\n",
        "\n",
        "topic_nuns = ['atmosphere_sound', 'chicken_menu', 'bad_service', 'pizza_menu', 'delivery', 'long_wait', 'drinks', 'wrong_marketing', 'dirty', 'rude_staff', 'burger_menu', 'over_priced', 'not_tasty', 'not_accessible', 'seasoning']\n",
        "\n",
        "model = pk.load(open(MODEL_FILE, 'rb'))\n",
        "vect = pk.load(open(VECT_FILE, 'rb'))\n",
        "\n",
        "def topic_search(text, nb_topic, model, vectorizer):\n",
        "  text = [text]\n",
        "  my_vertorizer = vect.transform(text)\n",
        "  my_model = model.transform(my_vertorizer)\n",
        "  topics = []\n",
        "  for i in np.argsort(my_model):\n",
        "    for j in range(nb_topic):\n",
        "      topics.append(topic_nuns[i[-j-1]])\n",
        "  return topics\n"
      ],
      "metadata": {
        "id": "tNgfo-ldVKS6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}